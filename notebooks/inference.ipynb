{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speechbrain as sb\n",
    "import torch\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\"  # Change this path to the path where you keep your data.\n",
    "\n",
    "PREP_DATA_FOLDER = f'{DATA_DIR}/prep_data/'\n",
    "\n",
    "RESULTS_FOLDER = f'{DATA_DIR}/results/'\n",
    "EXP_METADATA_FILE = f'{RESULTS_FOLDER}/exp_metadata.csv'\n",
    "PREP_SCORING_RESULTS_FILE = f'{RESULTS_FOLDER}/results_scoring_prep.csv'\n",
    "EPOCH_RESULTS_DIR = f'{RESULTS_FOLDER}/epoch_results'\n",
    "PARAMS_DIR= f'{RESULTS_FOLDER}/params'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = \"w2v2\"\n",
    "SCORING_TYPE=\"\"\n",
    "\n",
    "SCORING_HPARAM_FILE = f'hparams/scoring/{MODEL_TYPE}/train_{MODEL_TYPE}_so762{SCORING_TYPE}_scoring.yaml'\n",
    "SCORING_MODEL_DIR = f\"results/scoring/{MODEL_TYPE}/crdnn_{MODEL_TYPE}_so762{SCORING_TYPE}_scoring_aug_no_round_no_pre_train\"\n",
    "PRETRAINED_MODEL_DIR = f\"results/apr/{MODEL_TYPE}/crdnn_{MODEL_TYPE}_timit_apr/1234\"\n",
    "SCORING_HPARAM_FILE = f\"hparams/scoring/{MODEL_TYPE}/train_{MODEL_TYPE}_so762{SCORING_TYPE}_scoring.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "argv = [\n",
    "    SCORING_HPARAM_FILE,\n",
    "    \"--data_folder\", PREP_DATA_FOLDER,\n",
    "    \"--batch_size\", \"4\",\n",
    "    \"--pretrained_model_folder\", PRETRAINED_MODEL_DIR,\n",
    "    \"--use_augmentation\", \"True\",\n",
    "    \"--exp_folder\", SCORING_MODEL_DIR,\n",
    "    \"--exp_metadata_file\", EXP_METADATA_FILE,\n",
    "    \"--results_file\", PREP_SCORING_RESULTS_FILE,\n",
    "    \"--epoch_results_dir\", EPOCH_RESULTS_DIR,\n",
    "    \"--params_dir\", PARAMS_DIR\n",
    "    ]\n",
    "\n",
    "hparams_file, run_opts, overrides = sb.parse_arguments(argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperpyyaml import load_hyperpyyaml\n",
    "\n",
    "with open(hparams_file) as fin:\n",
    "    hparams = load_hyperpyyaml(fin, overrides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.brain import get_brain_class\n",
    "\n",
    "brain_class = get_brain_class(hparams)\n",
    "brain = brain_class(\n",
    "        modules=hparams[\"modules\"],\n",
    "        hparams=hparams,\n",
    "        run_opts=run_opts,\n",
    "        checkpointer=hparams[\"checkpointer\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_path = \"results/scoring/w2v2/crdnn_w2v2_so762_scoring_aug_no_round_no_pre_train/1234/save-1/CKPT+2024-01-30+08-10-42+00\"\n",
    "\n",
    "wav2vec2_ckpt_path = f'{ckpt_path}/wav2vec2.ckpt'\n",
    "model_ckpt_path = f'{ckpt_path}/model.ckpt'\n",
    "model_scorer_ckpt_path = f'{ckpt_path}/model_scorer.ckpt'\n",
    "\n",
    "wav2vec2_state_dict = torch.load(wav2vec2_ckpt_path)\n",
    "model_state_dict = torch.load(model_ckpt_path)\n",
    "model_scorer_state_dict = torch.load(model_scorer_ckpt_path)\n",
    "\n",
    "hparams[\"wav2vec2\"].load_state_dict(wav2vec2_state_dict)\n",
    "hparams[\"model\"].load_state_dict(model_state_dict)\n",
    "hparams[\"model_scorer\"].load_state_dict(model_scorer_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams[\"label_encoder_path\"] = \"results/scoring/w2v2/crdnn_w2v2_so762_scoring_aug_no_round_no_pre_train/1234/save/label_encoder.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder_path = hparams[\"label_encoder_path\"]\n",
    "label_encoder = sb.dataio.encoder.CTCTextEncoder.from_saved(label_encoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_prep_utils.dataset_preparation.dataio_prep import dataio_prep\n",
    "\n",
    "train_data, valid_data, test_data, label_encoder = dataio_prep(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "path = \"/home/tuyendv/E2E-R/lexicon\"\n",
    "lexicon = pd.read_csv(path, names=[\"word\", \"arpa\"], sep=\"\\t\")\n",
    "\n",
    "lexicon.dropna(inplace=True)\n",
    "lexicon[\"word\"] = lexicon.word.apply(lambda x: x.lower())\n",
    "lexicon[\"arpa\"] = lexicon.arpa.apply(lambda x: re.sub(\"\\d\", \"\", x).lower())\n",
    "\n",
    "lexicon.word.drop_duplicates(inplace=True)\n",
    "lexicon.set_index(\"word\", inplace=True)\n",
    "lexicon = lexicon.to_dict()[\"arpa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"/home/tuyendv/E2E-R/wav/mother.wav\"\n",
    "transcript = \"mother\"\n",
    "\n",
    "transcript = [lexicon[word] for word in transcript.split()]\n",
    "transcript = \" \".join(transcript)\n",
    "transcript = transcript.replace(\"ax\", \"ah\")\n",
    "\n",
    "phn_canonical_list = transcript.split()\n",
    "wavs = sb.dataio.dataio.read_audio(audio_path)\n",
    "\n",
    "phn_encoded_list = label_encoder.encode_sequence(phn_canonical_list)\n",
    "\n",
    "phn_canonical_encoded = torch.LongTensor(phn_encoded_list)\n",
    "phn_canonical_encoded_eos = torch.LongTensor(label_encoder.append_eos_index(phn_encoded_list))\n",
    "phn_canonical_encoded_bos = torch.LongTensor(label_encoder.prepend_bos_index(phn_encoded_list))\n",
    "\n",
    "wavs = wavs.unsqueeze(0).cuda()\n",
    "wav_lens = torch.tensor([wavs.shape[1]]).cuda()\n",
    "phns_canonical_bos = phn_canonical_encoded_bos.unsqueeze(0).cuda()\n",
    "phns_canonical_eos = phn_canonical_encoded_eos.unsqueeze(0).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9290, 0.8414, 0.8655, 0.7649, 0.5825]], device='cuda:0',\n",
      "       grad_fn=<SumBackward1>)\n",
      "tensor([[1.8581, 1.6829, 1.7310, 1.5297, 1.1649]], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "scores_pred, wav_lens = brain.infer(wavs, wav_lens, phns_canonical_bos, phns_canonical_eos)\n",
    "print(scores_pred)\n",
    "scores_pred = brain.rescale_scores(scores_pred)\n",
    "print(scores_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
