{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /data/codes/apa/train/\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "import json\n",
    "from pandarallel import pandarallel\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "import torchaudio\n",
    "import random\n",
    "import torch\n",
    "import re\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "pandarallel.initialize(nb_workers=8, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type2path = {\n",
    "    10: {\n",
    "        \"json_dir\": \"/data/metadata/apa-en/marking-data/10\",\n",
    "        \"audio_dir\": \"/data/audio/prep-submission-audio/apa-type-10\",\n",
    "        \"metadata_path\": \"/data/metadata/apa-en/merged-info/info_question_type-10_01082022_18092023.csv\",\n",
    "        \"out_metadata_path\": \"/data/metadata/stt-en/raw/vad-filtered-info_question_type-10_01082022_18092023.csv\"\n",
    "    },\n",
    "    12: {\n",
    "        \"json_dir\": \"/data/metadata/apa-en/marking-data/12\",\n",
    "        \"audio_dir\": \"/data/audio/prep-submission-audio/apa-type-12\",\n",
    "        \"metadata_path\": \"/data/metadata/apa-en/merged-info/info_question_type-12_01082022_18092023.csv\",\n",
    "        \"out_metadata_path\": \"/data/metadata/stt-en/raw/vad-filtered-info_question_type-12_01082022_18092023.csv\"\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_type_ = 12\n",
    "path_dict = type2path[_type_]\n",
    "\n",
    "in_audio_dir = path_dict[\"audio_dir\"]\n",
    "\n",
    "data_root_dir = \"/data/codes/apa/train/data\" \n",
    "data_name = os.path.basename(path_dict[\"metadata_path\"]).split(\".\")[0]\n",
    "data_dir = os.path.join(data_root_dir, data_name)\n",
    "\n",
    "out_metadata_path = path_dict[\"out_metadata_path\"]\n",
    "out_raw_json_path = f'{data_dir}/metadata-raw.jsonl'\n",
    "out_audio_dir = f'{data_dir}/wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"json_dir\": path_dict[\"json_dir\"],\n",
    "    \"audio_dir\": path_dict[\"audio_dir\"],\n",
    "    \"metadata_path\": path_dict[\"metadata_path\"],\n",
    "}\n",
    "\n",
    "metadata = pd.read_csv(hparams[\"metadata_path\"])\n",
    "\n",
    "metadata = metadata[metadata.word_count == 1.0]\n",
    "metadata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_audio(audio_id):\n",
    "    abs_path = os.path.join(hparams[\"audio_dir\"], f'{audio_id}.wav')\n",
    "    if not os.path.exists(abs_path):\n",
    "        return False\n",
    "    try:\n",
    "        wav, sr = torchaudio.load(abs_path)\n",
    "        if sr != 16000:\n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "is_exist =  metadata.id.parallel_apply(is_valid_audio)\n",
    "print(metadata.shape)\n",
    "metadata = metadata[is_exist]\n",
    "print(metadata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_file(id):\n",
    "    json_path = os.path.join(hparams[\"json_dir\"], f'{id}.json')\n",
    "    audio_path = os.path.join(hparams[\"audio_dir\"], f'{id}.wav')\n",
    "\n",
    "    try:\n",
    "        waveform, sr = librosa.load(audio_path, sr=16000)\n",
    "        duration = waveform.shape[0] / sr\n",
    "\n",
    "        with open(json_path, \"r\") as f:\n",
    "            content = json.load(f)\n",
    "        \n",
    "        segments = []\n",
    "        for utt_id, raw_utterance in enumerate(content[\"utterance\"]):\n",
    "            for word_id, word in enumerate(raw_utterance[\"words\"]):\n",
    "                segment = [\n",
    "                    word[\"start_time\"],\n",
    "                    word[\"end_time\"]\n",
    "                ]\n",
    "                \n",
    "                segments.append(segment)\n",
    "\n",
    "        results = {\n",
    "            \"segments\": segments,\n",
    "            \"duration\": duration\n",
    "        }\n",
    "        return results\n",
    "    \n",
    "    except:\n",
    "        return None\n",
    "\n",
    "parsed_segments = metadata.id.parallel_apply(parse_json_file)\n",
    "metadata = metadata[~parsed_segments.isna()]\n",
    "parsed_segments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_segments = parsed_segments[~parsed_segments.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_silence_segment(segments, duration):\n",
    "    silence_segments = []\n",
    "    \n",
    "    prev = None\n",
    "    start, end = None, None\n",
    "    for curr in segments:\n",
    "        if prev is None:\n",
    "            start = 0\n",
    "        else:\n",
    "            start = prev[1]\n",
    "        \n",
    "        end = curr[0]\n",
    "        prev = curr\n",
    "\n",
    "        if start == end:\n",
    "            continue\n",
    "\n",
    "        segment = [start, end]\n",
    "        silence_segments.append(segment)\n",
    "\n",
    "    silence_segments.append([curr[-1], duration])\n",
    "    return silence_segments\n",
    "\n",
    "silence_segments = parsed_segments.parallel_apply(\n",
    "    lambda row: get_silence_segment(\n",
    "        segments=row[\"segments\"], duration=row[\"duration\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": metadata[\"id\"],\n",
    "        \"segments\": silence_segments\n",
    "    }\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import hub\n",
    "import torchaudio\n",
    "import librosa\n",
    "\n",
    "class Voice_Activity_Detection():\n",
    "    def __init__(self, sample_rate=16000, device=\"cuda\"):\n",
    "        self.device = device\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "        self.model, self.utils = hub.load(\n",
    "            repo_or_dir=\"snakers4/silero-vad\", \n",
    "            model=\"silero_vad\", \n",
    "            force_reload=False, \n",
    "            onnx=False\n",
    "        )\n",
    "        self.model.to(device)\n",
    "\n",
    "        self.fn_get_speech_timestamps, self.fn_save_audio, \\\n",
    "            self.fn_read_audio, self.VADIterator, self.fn_collect_chunks = self.utils\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_speech_timestamps(self, segments, threshold=0.7):\n",
    "        timestamps = self.fn_get_speech_timestamps(\n",
    "            segments.to(self.device), self.model, threshold=threshold, sampling_rate=self.sample_rate)\n",
    "        \n",
    "        return timestamps\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def is_valid_segment(self, segment, threshold=0.7, min_duration=0.2):\n",
    "        timestamps = self.fn_get_speech_timestamps(\n",
    "            segment.to(self.device), self.model, threshold=threshold, sampling_rate=self.sample_rate)\n",
    "        \n",
    "        if len(timestamps) == 0:\n",
    "            return True\n",
    "        \n",
    "        is_speech_duration = 0\n",
    "        for segment in timestamps:\n",
    "            duration = segment[\"end\"] - segment[\"start\"]\n",
    "\n",
    "            is_speech_duration += duration\n",
    "\n",
    "        if (is_speech_duration/self.sample_rate) < min_duration:\n",
    "            return True\n",
    "                \n",
    "        return False\n",
    "    \n",
    "vad_model = Voice_Activity_Detection(device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_sample(id, segments, vad_model):\n",
    "    audio_path = os.path.join(hparams[\"audio_dir\"], f'{id}.wav')\n",
    "    waveform, sr = librosa.load(audio_path, sr=16000)\n",
    "\n",
    "    silences = []\n",
    "    for start, end in segments:\n",
    "        segment = waveform[int(start*sr): int(end*sr)]\n",
    "        segment = torch.from_numpy(segment)\n",
    "\n",
    "        silences.append(segment)\n",
    "    \n",
    "    silences = torch.concat(silences)\n",
    "    is_valid = vad_model.is_valid_segment(silences, threshold=0.7, min_duration=0.2)\n",
    "\n",
    "    return is_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import multiprocessing\n",
    "\n",
    "from concurrent.futures import (\n",
    "    ProcessPoolExecutor, \n",
    "    as_completed\n",
    ")\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "sr = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_models = dict()\n",
    "\n",
    "def init_model():\n",
    "    pid = multiprocessing.current_process().pid\n",
    "\n",
    "    vad_models[pid] = Voice_Activity_Detection(device=\"cpu\")\n",
    "\n",
    "def vad_process(df):\n",
    "    pid = multiprocessing.current_process().pid\n",
    "\n",
    "    model = vad_models[pid]\n",
    "    with torch.no_grad():\n",
    "        is_valid = df.progress_apply(\n",
    "            lambda row: is_valid_sample(\n",
    "                id=row[\"id\"], segments=row[\"segments\"], vad_model=model), axis=1\n",
    "            )\n",
    "\n",
    "    return is_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_process = 16\n",
    "num_sample_per_process = int(df.shape[0] / num_process) + 1\n",
    "\n",
    "params = []\n",
    "for i in range(num_process):\n",
    "    params.append(\n",
    "        df[i*num_sample_per_process: (i+1)*num_sample_per_process])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "futures = []\n",
    "with ProcessPoolExecutor(max_workers=num_process, initializer=init_model) as executor:\n",
    "    for param in params:\n",
    "        futures.append(\n",
    "            executor.submit(vad_process, param))\n",
    "\n",
    "results = [finished.result() for finished in as_completed(futures)]\n",
    "is_valid = pd.concat(results).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metadata[is_valid].shape)\n",
    "print(metadata[~is_valid].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = metadata[is_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.to_csv(out_metadata_path, index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
